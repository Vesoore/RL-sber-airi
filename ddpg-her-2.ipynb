{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db243473",
   "metadata": {
    "papermill": {
     "duration": 0.004847,
     "end_time": "2026-02-16T21:17:38.107201",
     "exception": false,
     "start_time": "2026-02-16T21:17:38.102354",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DDPG + Hindsight Experience Replay (HER)\n",
    "\n",
    "Implementation of **Deep Deterministic Policy Gradient (DDPG)** with\n",
    "**Hindsight Experience Replay (HER)** for goal-conditioned robotic\n",
    "manipulation tasks from [Gymnasium-Robotics](https://robotics.farama.org/).\n",
    "\n",
    "**References:**\n",
    "- Lillicrap et al., [Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971), 2015\n",
    "- Andrychowicz et al., [Hindsight Experience Replay](https://arxiv.org/abs/1707.01495), 2017\n",
    "\n",
    "**Experiments** (all on `FetchReach-v4`):\n",
    "\n",
    "| # | Run name | HER | Goal strategy |\n",
    "|---|----------|-----|---------------|\n",
    "| 1 | `DDPG` | ✗ | — |\n",
    "| 2 | `DDPG_HER_final` | ✓ | `final` |\n",
    "| 3 | `DDPG_HER_episode` | ✓ | `episode` |\n",
    "| 4 | `DDPG_HER_future` | ✓ | `future` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f66aa45",
   "metadata": {
    "papermill": {
     "duration": 0.003828,
     "end_time": "2026-02-16T21:17:38.114951",
     "exception": false,
     "start_time": "2026-02-16T21:17:38.111123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18599032",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:17:38.123899Z",
     "iopub.status.busy": "2026-02-16T21:17:38.123629Z",
     "iopub.status.idle": "2026-02-16T21:17:50.748603Z",
     "shell.execute_reply": "2026-02-16T21:17:50.747652Z"
    },
    "papermill": {
     "duration": 12.631597,
     "end_time": "2026-02-16T21:17:50.750366",
     "exception": false,
     "start_time": "2026-02-16T21:17:38.118769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.2/26.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.1/952.1 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.5/243.5 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "stable-baselines3 2.1.0 requires gymnasium<0.30,>=0.28.1, but you have gymnasium 1.2.3 which is incompatible.\r\n",
      "kaggle-environments 1.18.0 requires gymnasium==0.29.0, but you have gymnasium 1.2.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q \\\n",
    "    gymnasium gymnasium-robotics \"gymnasium[mujoco]\" \\\n",
    "    torch torchvision \\\n",
    "    matplotlib numpy wandb tqdm imageio moviepy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb452d8",
   "metadata": {
    "papermill": {
     "duration": 0.007546,
     "end_time": "2026-02-16T21:17:50.766092",
     "exception": false,
     "start_time": "2026-02-16T21:17:50.758546",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Weights & Biases Setup (Optional)\n",
    "\n",
    "Logging to [W&B](https://wandb.ai) is **optional**. If `WANDB_API_KEY` is\n",
    "set in the environment, metrics and evaluation videos will be logged\n",
    "automatically. Otherwise, training still works — results are printed to\n",
    "stdout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b8a99a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:17:50.783815Z",
     "iopub.status.busy": "2026-02-16T21:17:50.783129Z",
     "iopub.status.idle": "2026-02-16T21:17:53.376534Z",
     "shell.execute_reply": "2026-02-16T21:17:53.375616Z"
    },
    "papermill": {
     "duration": 2.603649,
     "end_time": "2026-02-16T21:17:53.378030",
     "exception": false,
     "start_time": "2026-02-16T21:17:50.774381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WANDB_API_KEY not set — logging disabled. Set the env var or run `wandb login` to enable.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "USE_WANDB = os.environ.get(\"WANDB_API_KEY\") is not None\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.login()\n",
    "else:\n",
    "    print(\"WANDB_API_KEY not set — logging disabled. \"\n",
    "          \"Set the env var or run `wandb login` to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274602e8",
   "metadata": {
    "papermill": {
     "duration": 0.007534,
     "end_time": "2026-02-16T21:17:53.393680",
     "exception": false,
     "start_time": "2026-02-16T21:17:53.386146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d7f56ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:17:53.410135Z",
     "iopub.status.busy": "2026-02-16T21:17:53.409847Z",
     "iopub.status.idle": "2026-02-16T21:17:57.297286Z",
     "shell.execute_reply": "2026-02-16T21:17:57.296694Z"
    },
    "papermill": {
     "duration": 3.897693,
     "end_time": "2026-02-16T21:17:57.298940",
     "exception": false,
     "start_time": "2026-02-16T21:17:53.401247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "from gymnasium import Wrapper\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gym.register_envs(gymnasium_robotics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f863aaf",
   "metadata": {
    "papermill": {
     "duration": 0.007554,
     "end_time": "2026-02-16T21:17:57.314535",
     "exception": false,
     "start_time": "2026-02-16T21:17:57.306981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Neural Network Building Blocks\n",
    "\n",
    "`ConvertedSigmoid` squashes network output from $(0, 1)$ to an arbitrary\n",
    "$[\\text{low}, \\text{high}]$ range — used as the actor's output activation\n",
    "so that actions stay within environment bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9da521f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:17:57.330856Z",
     "iopub.status.busy": "2026-02-16T21:17:57.330502Z",
     "iopub.status.idle": "2026-02-16T21:17:57.337532Z",
     "shell.execute_reply": "2026-02-16T21:17:57.336953Z"
    },
    "papermill": {
     "duration": 0.016742,
     "end_time": "2026-02-16T21:17:57.338789",
     "exception": false,
     "start_time": "2026-02-16T21:17:57.322047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvertedSigmoid(nn.Module):\n",
    "    \"\"\"Sigmoid rescaled to [low_bound, high_bound].\"\"\"\n",
    "\n",
    "    def __init__(self, low_bound, high_bound):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"low_bound\", torch.tensor(low_bound))\n",
    "        self.register_buffer(\"scale\", torch.tensor(high_bound - low_bound))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(x) * self.scale + self.low_bound\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple multi-layer perceptron.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim,\n",
    "                 activation=nn.ReLU, output_activation=nn.Identity()):\n",
    "        super().__init__()\n",
    "        if isinstance(hidden_dims, int):\n",
    "            hidden_dims = [hidden_dims]\n",
    "\n",
    "        layers = [nn.Linear(input_dim, hidden_dims[0]), activation()]\n",
    "        for d_in, d_out in zip(hidden_dims[:-1], hidden_dims[1:]):\n",
    "            layers += [nn.Linear(d_in, d_out), activation()]\n",
    "        layers += [nn.Linear(hidden_dims[-1], output_dim), output_activation]\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783fbf83",
   "metadata": {
    "papermill": {
     "duration": 0.007495,
     "end_time": "2026-02-16T21:17:57.353819",
     "exception": false,
     "start_time": "2026-02-16T21:17:57.346324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. DDPG Agent\n",
    "\n",
    "`DDPG` implements the vanilla algorithm; `GoalDDPG` extends it by\n",
    "concatenating the desired goal to both actor and critic inputs,\n",
    "making the policy goal-conditioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fed3041e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:17:57.370478Z",
     "iopub.status.busy": "2026-02-16T21:17:57.370240Z",
     "iopub.status.idle": "2026-02-16T21:17:57.385856Z",
     "shell.execute_reply": "2026-02-16T21:17:57.385235Z"
    },
    "papermill": {
     "duration": 0.025741,
     "end_time": "2026-02-16T21:17:57.387238",
     "exception": false,
     "start_time": "2026-02-16T21:17:57.361497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DDPG(nn.Module):\n",
    "    \"\"\"Deep Deterministic Policy Gradient (Lillicrap et al., 2015).\"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, min_action_values, max_action_values,\n",
    "                 hidden_dims, exploration_std, gamma,\n",
    "                 target_exponential_averaging, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.tau = target_exponential_averaging\n",
    "        self.exploration_std = exploration_std\n",
    "\n",
    "        action_dim = min_action_values.shape\n",
    "        self.min_action_values = torch.tensor(min_action_values, device=device)\n",
    "        self.max_action_values = torch.tensor(max_action_values, device=device)\n",
    "\n",
    "        assert len(obs_dim) == 1, \"obs_dim must be 1-D\"\n",
    "        assert len(action_dim) == 1, \"action_dim must be 1-D\"\n",
    "\n",
    "        # Online networks\n",
    "        self.critic = MLP(obs_dim[0] + action_dim[0], hidden_dims, 1).to(device)\n",
    "        self.actor = MLP(\n",
    "            obs_dim[0], hidden_dims, action_dim[0],\n",
    "            output_activation=ConvertedSigmoid(min_action_values, max_action_values),\n",
    "        ).to(device)\n",
    "\n",
    "        # Target networks (initialised as copies)\n",
    "        self._target_critic = MLP(obs_dim[0] + action_dim[0], hidden_dims, 1).to(device)\n",
    "        self._target_actor = MLP(\n",
    "            obs_dim[0], hidden_dims, action_dim[0],\n",
    "            output_activation=ConvertedSigmoid(min_action_values, max_action_values),\n",
    "        ).to(device)\n",
    "        self._target_critic.load_state_dict(self.critic.state_dict())\n",
    "        self._target_actor.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "    # ---- action selection ----\n",
    "\n",
    "    def _apply_exploration_noise(self, action):\n",
    "        noise = torch.randn_like(action, device=self.device) * self.exploration_std\n",
    "        return torch.clamp(action + noise,\n",
    "                           min=self.min_action_values,\n",
    "                           max=self.max_action_values)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, obs, training=True):\n",
    "        obs = torch.from_numpy(obs).to(self.device)\n",
    "        action = self.actor(obs)\n",
    "        if training:\n",
    "            action = self._apply_exploration_noise(action)\n",
    "        return action.cpu().numpy()\n",
    "\n",
    "    # ---- batch decomposition helpers ----\n",
    "\n",
    "    def _split_batch(self, batch):\n",
    "        cur = {\"state\": batch[\"state\"], \"action\": batch[\"action\"]}\n",
    "        nxt = {\"state\": batch[\"next_state\"], \"action\": None}\n",
    "        return cur, nxt\n",
    "\n",
    "    def _actor_input(self, d):\n",
    "        return d[\"state\"]\n",
    "\n",
    "    def _critic_input(self, d):\n",
    "        return torch.cat([d[\"state\"], d[\"action\"]], dim=-1)\n",
    "\n",
    "    # ---- losses ----\n",
    "\n",
    "    def get_critic_loss(self, batch):\n",
    "        reward, terminated = batch[\"reward\"], batch[\"terminated\"]\n",
    "        cur, nxt = self._split_batch(batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            nxt[\"action\"] = self._target_actor(self._actor_input(nxt))\n",
    "            target = reward + self.gamma * (1 - terminated.int()) \\\n",
    "                     * self._target_critic(self._critic_input(nxt))\n",
    "\n",
    "        q_values = self.critic(self._critic_input(cur))\n",
    "        return torch.mean((q_values - target) ** 2)\n",
    "\n",
    "    def get_actor_loss(self, batch):\n",
    "        cur, _ = self._split_batch(batch)\n",
    "        cur[\"action\"] = self.actor(self._actor_input(cur))\n",
    "        return -self.critic(self._critic_input(cur)).mean()\n",
    "\n",
    "    # ---- target update ----\n",
    "\n",
    "    def update_target_networks(self):\n",
    "        for target, online in [(self._target_critic, self.critic),\n",
    "                               (self._target_actor, self.actor)]:\n",
    "            for tp, op in zip(target.parameters(), online.parameters()):\n",
    "                tp.data.mul_(self.tau).add_(op.data, alpha=1 - self.tau)\n",
    "\n",
    "    # ---- parameter groups (for separate optimisers) ----\n",
    "\n",
    "    def critic_parameters(self):\n",
    "        return list(self.critic.parameters())\n",
    "\n",
    "    def actor_parameters(self):\n",
    "        return list(self.actor.parameters())\n",
    "\n",
    "\n",
    "class GoalDDPG(DDPG):\n",
    "    \"\"\"Goal-conditioned DDPG: observation and goal are concatenated.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, goal_dim, min_action_values, max_action_values,\n",
    "                 hidden_dims, exploration_std, gamma,\n",
    "                 target_exponential_averaging, device, seed):\n",
    "        assert len(obs_dim) == 1 and len(goal_dim) == 1\n",
    "        super().__init__(\n",
    "            obs_dim=(obs_dim[0] + goal_dim[0],),\n",
    "            min_action_values=min_action_values,\n",
    "            max_action_values=max_action_values,\n",
    "            hidden_dims=hidden_dims,\n",
    "            exploration_std=exploration_std,\n",
    "            gamma=gamma,\n",
    "            target_exponential_averaging=target_exponential_averaging,\n",
    "            device=device,\n",
    "        )\n",
    "        self.np_rng = np.random.default_rng(seed)\n",
    "\n",
    "    def _split_batch(self, batch):\n",
    "        cur = {\"state\": batch[\"state\"], \"action\": batch[\"action\"],\n",
    "               \"goal\": batch[\"desired_goal\"]}\n",
    "        nxt = {\"state\": batch[\"next_state\"], \"action\": None,\n",
    "               \"goal\": batch[\"desired_goal\"]}\n",
    "        return cur, nxt\n",
    "\n",
    "    def _actor_input(self, d):\n",
    "        return torch.cat([d[\"state\"], d[\"goal\"]], dim=-1)\n",
    "\n",
    "    def _critic_input(self, d):\n",
    "        return torch.cat([d[\"state\"], d[\"action\"], d[\"goal\"]], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed1e3b",
   "metadata": {
    "papermill": {
     "duration": 0.007573,
     "end_time": "2026-02-16T21:17:57.402373",
     "exception": false,
     "start_time": "2026-02-16T21:17:57.394800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Environment Wrapper\n",
    "\n",
    "Converts observations to `float32` and wraps scalar reward / flags into\n",
    "arrays for uniform downstream handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fcf629c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:17:57.421164Z",
     "iopub.status.busy": "2026-02-16T21:17:57.420915Z",
     "iopub.status.idle": "2026-02-16T21:17:57.425898Z",
     "shell.execute_reply": "2026-02-16T21:17:57.425362Z"
    },
    "papermill": {
     "duration": 0.014834,
     "end_time": "2026-02-16T21:17:57.427289",
     "exception": false,
     "start_time": "2026-02-16T21:17:57.412455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GoalEnvWrapper(Wrapper):\n",
    "    \"\"\"Thin wrapper for Gymnasium goal-conditioned environments.\n",
    "\n",
    "    - Casts observations to float32.\n",
    "    - Wraps scalar reward / terminated / truncated into 1-D arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        obs = {k: v.astype(np.float32) for k, v in obs.items()}\n",
    "        reward = np.array([reward], dtype=np.float32)\n",
    "        terminated = np.array([terminated])\n",
    "        truncated = np.array([truncated])\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        obs, info = self.env.reset(seed=seed, options=options)\n",
    "        obs = {k: v.astype(np.float32) for k, v in obs.items()}\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907823e4",
   "metadata": {
    "papermill": {
     "duration": 0.007595,
     "end_time": "2026-02-16T21:17:57.442440",
     "exception": false,
     "start_time": "2026-02-16T21:17:57.434845",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Replay Buffers\n",
    "\n",
    "- `ReplayBuffer` — standard trajectory-based replay buffer.\n",
    "- `HERReplayBuffer` — extends the base buffer with **Hindsight Experience\n",
    "  Replay**: at sampling time a fraction of transitions gets new (hindsight)\n",
    "  goals, and the corresponding rewards / termination flags are recomputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d13b5100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:17:57.459215Z",
     "iopub.status.busy": "2026-02-16T21:17:57.458939Z",
     "iopub.status.idle": "2026-02-16T21:17:57.472541Z",
     "shell.execute_reply": "2026-02-16T21:17:57.471939Z"
    },
    "papermill": {
     "duration": 0.023583,
     "end_time": "2026-02-16T21:17:57.473891",
     "exception": false,
     "start_time": "2026-02-16T21:17:57.450308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "REPLAY_KEYS = [\n",
    "    \"state\", \"next_state\", \"action\",\n",
    "    \"terminated\", \"truncated\", \"reward\",\n",
    "    \"achieved_goal\", \"desired_goal\",\n",
    "]\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Trajectory-level replay buffer stored in RAM.\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, max_size, keys, seed, device):\n",
    "        self.memory: list[list[dict]] = []\n",
    "        self.size = 0\n",
    "        self.max_size = max_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.keys = keys\n",
    "\n",
    "        # Maps global index → (trajectory_idx, timestep_idx)\n",
    "        self._index_map: dict[int, tuple[int, int]] = {}\n",
    "\n",
    "    # ---- storage ----\n",
    "\n",
    "    def add_trajectory(self, trajectory):\n",
    "        # Evict oldest trajectories if necessary\n",
    "        evict_len, evict_count = 0, 0\n",
    "        while self.size + len(trajectory) - evict_len > self.max_size:\n",
    "            evict_len += len(self.memory[evict_count])\n",
    "            evict_count += 1\n",
    "\n",
    "        if evict_count > 0:\n",
    "            self.memory = self.memory[evict_count:]\n",
    "            new_map = {}\n",
    "            for g_idx, (t_idx, e_idx) in self._index_map.items():\n",
    "                if t_idx >= evict_count:\n",
    "                    new_map[g_idx - evict_len] = (t_idx - evict_count, e_idx)\n",
    "            self._index_map = new_map\n",
    "            self.size -= evict_len\n",
    "\n",
    "        next_traj_idx = (self._index_map[self.size - 1][0] + 1) if self.size else 0\n",
    "        for i in range(len(trajectory)):\n",
    "            self._index_map[self.size + i] = (next_traj_idx, i)\n",
    "        self.size += len(trajectory)\n",
    "        self.memory.append(trajectory)\n",
    "\n",
    "    # ---- sampling ----\n",
    "\n",
    "    def sample_batch(self, batch_size=None):\n",
    "        bs = batch_size or self.batch_size\n",
    "        idxs = self.rng.choice(self.size, size=bs)\n",
    "        return self._fetch(idxs)\n",
    "\n",
    "    def _fetch(self, idxs):\n",
    "        out = {k: [] for k in self.keys}\n",
    "        for i in idxs:\n",
    "            t_idx, ts_idx = self._index_map[i]\n",
    "            step = self.memory[t_idx][ts_idx]\n",
    "            for k in self.keys:\n",
    "                out[k].append(torch.from_numpy(step[k]))\n",
    "        return {k: torch.stack(out[k]).to(self.device) for k in self.keys}\n",
    "\n",
    "    def get_trajectory(self, global_idx):\n",
    "        t_idx, local_idx = self._index_map[global_idx]\n",
    "        return self.memory[t_idx], local_idx\n",
    "\n",
    "\n",
    "class HERReplayBuffer(ReplayBuffer):\n",
    "    \"\"\"Replay buffer with Hindsight Experience Replay.\n",
    "\n",
    "    Supports three goal-relabelling strategies:\n",
    "    - ``future``: sample from future timesteps in the same episode.\n",
    "    - ``final``:  always use the last achieved goal of the episode.\n",
    "    - ``episode``: sample uniformly from the entire episode.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, max_size, keys, seed, device,\n",
    "                 reward_fn, terminated_fn,\n",
    "                 goal_strategy=\"future\", relabel_prob=0.5):\n",
    "        super().__init__(batch_size, max_size, keys, seed, device)\n",
    "        self.reward_fn = reward_fn\n",
    "        self.terminated_fn = terminated_fn\n",
    "        self.goal_strategy = goal_strategy\n",
    "        self.relabel_prob = relabel_prob\n",
    "\n",
    "    def _sample_goal(self, trajectory, local_idx):\n",
    "        T = len(trajectory)\n",
    "        if self.goal_strategy == \"future\":\n",
    "            t = self.rng.integers(local_idx + 1, T) if local_idx < T - 1 else T - 1\n",
    "        elif self.goal_strategy == \"final\":\n",
    "            t = T - 1\n",
    "        elif self.goal_strategy == \"episode\":\n",
    "            t = self.rng.integers(0, T)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown goal strategy: {self.goal_strategy}\")\n",
    "        return trajectory[t][\"achieved_goal\"]\n",
    "\n",
    "    def sample_batch(self, batch_size=None):\n",
    "        bs = batch_size or self.batch_size\n",
    "        idxs = self.rng.choice(self.size, size=bs, replace=False)\n",
    "        batch = self._fetch(idxs)\n",
    "\n",
    "        mask = self.rng.random(bs) < self.relabel_prob\n",
    "        for i in np.where(mask)[0]:\n",
    "            traj, local = self.get_trajectory(idxs[i])\n",
    "            if len(traj) == 0:\n",
    "                continue\n",
    "\n",
    "            new_goal = self._sample_goal(traj, local)\n",
    "            batch[\"desired_goal\"][i] = torch.from_numpy(new_goal).to(self.device)\n",
    "\n",
    "            achieved = batch[\"achieved_goal\"][i].cpu().numpy()\n",
    "            batch[\"reward\"][i] = torch.tensor(\n",
    "                [self.reward_fn(achieved, new_goal, {})],\n",
    "                dtype=torch.float32, device=self.device,\n",
    "            )\n",
    "            batch[\"terminated\"][i] = torch.tensor(\n",
    "                [self.terminated_fn(achieved, new_goal, {})],\n",
    "                dtype=torch.bool, device=self.device,\n",
    "            )\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fee337e",
   "metadata": {
    "papermill": {
     "duration": 0.007559,
     "end_time": "2026-02-16T21:17:57.489285",
     "exception": false,
     "start_time": "2026-02-16T21:17:57.481726",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f801d8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:17:57.505478Z",
     "iopub.status.busy": "2026-02-16T21:17:57.505259Z",
     "iopub.status.idle": "2026-02-16T21:17:57.562150Z",
     "shell.execute_reply": "2026-02-16T21:17:57.561345Z"
    },
    "papermill": {
     "duration": 0.066856,
     "end_time": "2026-02-16T21:17:57.563600",
     "exception": false,
     "start_time": "2026-02-16T21:17:57.496744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ee1e24d6d30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environment\n",
    "EPISODE_LENGTH = 50\n",
    "SEED = 42\n",
    "\n",
    "# Validation\n",
    "VAL_EPISODES = 10\n",
    "WINDOW_SIZE = 20\n",
    "\n",
    "# Replay buffer\n",
    "BATCH_SIZE = 256\n",
    "MAX_BUFFER_SIZE = 1_000_000\n",
    "\n",
    "# Exploration\n",
    "PREHEAT_STEPS = 10_000\n",
    "EXPLORATION_STD = 0.2\n",
    "RANDOM_ACTION_PROB = 0.3\n",
    "\n",
    "# Network / optimisation\n",
    "HIDDEN_DIMS = [256, 256, 256]\n",
    "GAMMA = 0.99\n",
    "TAU = 0.95  # target network EMA coefficient\n",
    "UPDATE_FREQUENCY = 2\n",
    "PATIENCE = 10  # early stopping: max evals without improvement\n",
    "LR_ACTOR = 1e-4\n",
    "LR_CRITIC = 1e-3\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f665f1c",
   "metadata": {
    "papermill": {
     "duration": 0.007978,
     "end_time": "2026-02-16T21:17:57.579708",
     "exception": false,
     "start_time": "2026-02-16T21:17:57.571730",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43fda03f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:17:57.596906Z",
     "iopub.status.busy": "2026-02-16T21:17:57.596642Z",
     "iopub.status.idle": "2026-02-16T21:17:57.606011Z",
     "shell.execute_reply": "2026-02-16T21:17:57.605337Z"
    },
    "papermill": {
     "duration": 0.019965,
     "end_time": "2026-02-16T21:17:57.607381",
     "exception": false,
     "start_time": "2026-02-16T21:17:57.587416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def uniform_random_action(env):\n",
    "    \"\"\"Sample a uniform random action within the environment bounds.\"\"\"\n",
    "    low, high = env.action_space.low, env.action_space.high\n",
    "    return np.random.rand(*low.shape).astype(np.float32) * (high - low) + low\n",
    "\n",
    "\n",
    "def make_transition(obs, action, reward, terminated, truncated, next_obs):\n",
    "    \"\"\"Pack a single transition into a dict for the replay buffer.\"\"\"\n",
    "    return {\n",
    "        \"state\": obs[\"observation\"],\n",
    "        \"action\": action,\n",
    "        \"reward\": reward,\n",
    "        \"terminated\": terminated,\n",
    "        \"truncated\": truncated,\n",
    "        \"next_state\": next_obs[\"observation\"],\n",
    "        \"achieved_goal\": obs[\"achieved_goal\"],\n",
    "        \"desired_goal\": obs[\"desired_goal\"],\n",
    "    }\n",
    "\n",
    "\n",
    "def fill_buffer_random(env, buffer, n_steps):\n",
    "    \"\"\"Pre-fill the replay buffer with random-policy transitions.\"\"\"\n",
    "    obs = env.reset()\n",
    "    trajectory = []\n",
    "    for _ in range(n_steps):\n",
    "        action = uniform_random_action(env)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        trajectory.append(make_transition(obs, action, reward, terminated, truncated, next_obs))\n",
    "        obs = next_obs\n",
    "        if terminated[0] or truncated[0]:\n",
    "            buffer.add_trajectory(trajectory)\n",
    "            trajectory = []\n",
    "            obs = env.reset()\n",
    "    if trajectory:\n",
    "        buffer.add_trajectory(trajectory)\n",
    "\n",
    "\n",
    "def make_buffer(env, batch_size, max_size, seed, device, use_her=False, goal_strategy=\"future\"):\n",
    "    \"\"\"Create a replay buffer (optionally with HER).\"\"\"\n",
    "    if not use_her:\n",
    "        return ReplayBuffer(batch_size, max_size, REPLAY_KEYS, seed, device)\n",
    "\n",
    "    reward_fn = lambda ag, g, _: env.unwrapped.compute_reward(ag, g, None)\n",
    "    term_fn = lambda ag, g, _: env.unwrapped.compute_terminated(ag, g, None)\n",
    "    return HERReplayBuffer(\n",
    "        batch_size, max_size, REPLAY_KEYS, seed, device,\n",
    "        reward_fn=reward_fn, terminated_fn=term_fn,\n",
    "        goal_strategy=goal_strategy,\n",
    "    )\n",
    "\n",
    "\n",
    "class RunningMean:\n",
    "    \"\"\"Sliding-window mean calculator.\"\"\"\n",
    "\n",
    "    def __init__(self, window_size):\n",
    "        self._buf = []\n",
    "        self._max = window_size\n",
    "\n",
    "    def update(self, value):\n",
    "        self._buf.append(value)\n",
    "        if len(self._buf) > self._max:\n",
    "            self._buf.pop(0)\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return float(np.mean(self._buf)) if self._buf else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd4244",
   "metadata": {
    "papermill": {
     "duration": 0.009274,
     "end_time": "2026-02-16T21:17:57.624443",
     "exception": false,
     "start_time": "2026-02-16T21:17:57.615169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10. Training Loop\n",
    "\n",
    "Single function that trains a goal-conditioned DDPG agent with optional\n",
    "HER replay buffer. Logs metrics and evaluation videos to W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "923da420",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:17:57.641916Z",
     "iopub.status.busy": "2026-02-16T21:17:57.641657Z",
     "iopub.status.idle": "2026-02-16T21:17:57.654854Z",
     "shell.execute_reply": "2026-02-16T21:17:57.654302Z"
    },
    "papermill": {
     "duration": 0.023877,
     "end_time": "2026-02-16T21:17:57.656274",
     "exception": false,
     "start_time": "2026-02-16T21:17:57.632397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(agent, optim_actor, optim_critic, replay_buffer, env, *,\n",
    "          run_name, n_episodes=4_000, eval_every=100, patience=10):\n",
    "    \"\"\"Train a GoalDDPG agent and log results to Weights & Biases.\n",
    "\n",
    "    Training stops early if eval success rate does not improve\n",
    "    for ``patience`` consecutive evaluation cycles.\n",
    "    \"\"\"\n",
    "\n",
    "    if USE_WANDB:\n",
    "        wandb.init(project=\"DDPG-HER\", name=run_name, config={\n",
    "        \"episodes\": n_episodes,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"gamma\": GAMMA,\n",
    "        \"tau\": TAU,\n",
    "        \"lr_actor\": LR_ACTOR,\n",
    "        \"lr_critic\": LR_CRITIC,\n",
    "        \"exploration_std\": EXPLORATION_STD,\n",
    "        \"hidden_dims\": HIDDEN_DIMS,\n",
    "        \"preheat_steps\": PREHEAT_STEPS,\n",
    "        \"buffer_type\": type(replay_buffer).__name__,\n",
    "        })\n",
    "\n",
    "    success_tracker = RunningMean(WINDOW_SIZE)\n",
    "    step = 0\n",
    "    best_eval_sr = -1.0\n",
    "    evals_without_improvement = 0\n",
    "\n",
    "    try:\n",
    "        for episode in range(n_episodes):\n",
    "            agent.train()\n",
    "            critic_losses, actor_losses = [], []\n",
    "            obs = env.reset()\n",
    "            trajectory = []\n",
    "            done = False\n",
    "            episode_success = False\n",
    "\n",
    "            # ---- Collect & learn ----\n",
    "            while not done:\n",
    "                state_goal = np.concatenate([obs[\"observation\"], obs[\"desired_goal\"]])\n",
    "                action = agent.act(state_goal, training=True)\n",
    "                if np.random.random() < RANDOM_ACTION_PROB:\n",
    "                    action = uniform_random_action(env)\n",
    "\n",
    "                next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "                done = terminated[0] or truncated[0]\n",
    "                if info.get(\"is_success\", False):\n",
    "                    episode_success = True\n",
    "\n",
    "                trajectory.append(\n",
    "                    make_transition(obs, action, reward, terminated, truncated, next_obs)\n",
    "                )\n",
    "\n",
    "                # Gradient step\n",
    "                batch = replay_buffer.sample_batch()\n",
    "\n",
    "                optim_critic.zero_grad()\n",
    "                critic_loss = agent.get_critic_loss(batch)\n",
    "                critic_loss.backward()\n",
    "                optim_critic.step()\n",
    "                critic_losses.append(critic_loss.item())\n",
    "\n",
    "                if step % UPDATE_FREQUENCY == 0:\n",
    "                    optim_actor.zero_grad()\n",
    "                    actor_loss = agent.get_actor_loss(batch)\n",
    "                    actor_loss.backward()\n",
    "                    optim_actor.step()\n",
    "                    agent.update_target_networks()\n",
    "                    actor_losses.append(actor_loss.item())\n",
    "\n",
    "                obs = next_obs\n",
    "                step += 1\n",
    "\n",
    "            replay_buffer.add_trajectory(trajectory)\n",
    "            success_tracker.update(episode_success)\n",
    "\n",
    "            metrics = {\n",
    "                \"actor_loss\": np.mean(actor_losses) if actor_losses else 0,\n",
    "                \"critic_loss\": np.mean(critic_losses) if critic_losses else 0,\n",
    "                \"train_success_rate\": success_tracker.mean,\n",
    "            }\n",
    "\n",
    "            # ---- Evaluation ----\n",
    "            if episode % eval_every == 0:\n",
    "                agent.eval()\n",
    "                eval_successes = []\n",
    "                video_frames = []\n",
    "\n",
    "                for val_ep in range(VAL_EPISODES):\n",
    "                    obs_val = env.reset()\n",
    "                    done_val = False\n",
    "                    success = False\n",
    "\n",
    "                    while not done_val:\n",
    "                        if val_ep == 0:  # record video for the first eval episode\n",
    "                            video_frames.append(env.render().astype(np.uint8))\n",
    "\n",
    "                        sg = np.concatenate([obs_val[\"observation\"],\n",
    "                                             obs_val[\"desired_goal\"]])\n",
    "                        action = agent.act(sg, training=False)\n",
    "                        obs_val, _, term, trunc, info_val = env.step(action)\n",
    "                        done_val = term[0] or trunc[0]\n",
    "                        if info_val.get(\"is_success\", False):\n",
    "                            success = True\n",
    "\n",
    "                    eval_successes.append(success)\n",
    "\n",
    "                eval_sr = np.mean(eval_successes)\n",
    "                metrics[\"eval_success_rate\"] = eval_sr\n",
    "\n",
    "                if video_frames and USE_WANDB:\n",
    "                    video_np = np.array(video_frames, dtype=np.uint8)\n",
    "                    wandb.log({\"eval_video\": wandb.Video(video_np, fps=15, format=\"mp4\")},\n",
    "                              step=episode)\n",
    "\n",
    "                # Early stopping check\n",
    "                if eval_sr > best_eval_sr:\n",
    "                    best_eval_sr = eval_sr\n",
    "                    evals_without_improvement = 0\n",
    "                else:\n",
    "                    evals_without_improvement += 1\n",
    "\n",
    "                print(f\"Episode {episode:>5d} | \"\n",
    "                      f\"Train SR {success_tracker.mean:.3f} | \"\n",
    "                      f\"Eval SR {eval_sr:.3f} | \"\n",
    "                      f\"Best {best_eval_sr:.3f} | \"\n",
    "                      f\"Patience {evals_without_improvement}/{patience}\")\n",
    "\n",
    "                if evals_without_improvement >= patience:\n",
    "                    print(f\"Early stopping at episode {episode} \"\n",
    "                          f\"(no improvement for {patience} evals)\")\n",
    "                    break\n",
    "\n",
    "            if USE_WANDB:\n",
    "                wandb.log(metrics, step=episode)\n",
    "\n",
    "    except Exception:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        if USE_WANDB:\n",
    "            wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589c3656",
   "metadata": {
    "papermill": {
     "duration": 0.007886,
     "end_time": "2026-02-16T21:17:57.672132",
     "exception": false,
     "start_time": "2026-02-16T21:17:57.664246",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 11. MuJoCo Rendering Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4feba5a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:17:57.689089Z",
     "iopub.status.busy": "2026-02-16T21:17:57.688797Z",
     "iopub.status.idle": "2026-02-16T21:17:57.692205Z",
     "shell.execute_reply": "2026-02-16T21:17:57.691605Z"
    },
    "papermill": {
     "duration": 0.01358,
     "end_time": "2026-02-16T21:17:57.693604",
     "exception": false,
     "start_time": "2026-02-16T21:17:57.680024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"MUJOCO_GL\"] = \"egl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fe125f",
   "metadata": {
    "papermill": {
     "duration": 0.008026,
     "end_time": "2026-02-16T21:17:57.709422",
     "exception": false,
     "start_time": "2026-02-16T21:17:57.701396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 12. Experiment Runner\n",
    "\n",
    "Helper that creates the environment, agent, optimisers, and replay buffer,\n",
    "fills the buffer with random transitions, then runs training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1b0502a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:17:57.726416Z",
     "iopub.status.busy": "2026-02-16T21:17:57.726169Z",
     "iopub.status.idle": "2026-02-16T21:17:57.731847Z",
     "shell.execute_reply": "2026-02-16T21:17:57.731272Z"
    },
    "papermill": {
     "duration": 0.015501,
     "end_time": "2026-02-16T21:17:57.733152",
     "exception": false,
     "start_time": "2026-02-16T21:17:57.717651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiment(run_name, use_her=False, goal_strategy=\"future\"):\n",
    "    \"\"\"Initialise everything and run a single training experiment.\"\"\"\n",
    "\n",
    "    env = gym.make(\"FetchReach-v4\", render_mode=\"rgb_array\")\n",
    "    env = GoalEnvWrapper(env)\n",
    "    env.reset(seed=SEED)\n",
    "\n",
    "    obs_shape = env.unwrapped.observation_space[\"observation\"].shape\n",
    "    goal_shape = env.unwrapped.observation_space[\"desired_goal\"].shape\n",
    "\n",
    "    agent = GoalDDPG(\n",
    "        obs_dim=obs_shape,\n",
    "        goal_dim=goal_shape,\n",
    "        min_action_values=env.action_space.low,\n",
    "        max_action_values=env.action_space.high,\n",
    "        hidden_dims=HIDDEN_DIMS,\n",
    "        exploration_std=EXPLORATION_STD,\n",
    "        gamma=GAMMA,\n",
    "        target_exponential_averaging=TAU,\n",
    "        device=DEVICE,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    optim_actor = torch.optim.Adam(agent.actor_parameters(), lr=LR_ACTOR)\n",
    "    optim_critic = torch.optim.Adam(agent.critic_parameters(), lr=LR_CRITIC)\n",
    "\n",
    "    buffer = make_buffer(\n",
    "        env, BATCH_SIZE, MAX_BUFFER_SIZE, SEED, DEVICE,\n",
    "        use_her=use_her, goal_strategy=goal_strategy,\n",
    "    )\n",
    "    fill_buffer_random(env, buffer, PREHEAT_STEPS)\n",
    "\n",
    "    train(agent, optim_actor, optim_critic, buffer, env,\n",
    "          run_name=run_name, patience=PATIENCE)\n",
    "    env.close()\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9656521",
   "metadata": {
    "papermill": {
     "duration": 0.007644,
     "end_time": "2026-02-16T21:17:57.748517",
     "exception": false,
     "start_time": "2026-02-16T21:17:57.740873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 13. Experiments\n",
    "\n",
    "### 13.1 Baseline — DDPG without HER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e26877fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:17:57.765534Z",
     "iopub.status.busy": "2026-02-16T21:17:57.764904Z",
     "iopub.status.idle": "2026-02-16T21:54:26.092706Z",
     "shell.execute_reply": "2026-02-16T21:54:26.091961Z"
    },
    "papermill": {
     "duration": 2188.347464,
     "end_time": "2026-02-16T21:54:26.103939",
     "exception": false,
     "start_time": "2026-02-16T21:17:57.756475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode     0 | Train SR 0.000 | Eval SR 0.100 | Best 0.100 | Patience 0/10\n",
      "Episode   100 | Train SR 0.000 | Eval SR 0.100 | Best 0.100 | Patience 1/10\n",
      "Episode   200 | Train SR 0.100 | Eval SR 0.200 | Best 0.200 | Patience 0/10\n",
      "Episode   300 | Train SR 0.200 | Eval SR 0.000 | Best 0.200 | Patience 1/10\n",
      "Episode   400 | Train SR 0.150 | Eval SR 0.100 | Best 0.200 | Patience 2/10\n",
      "Episode   500 | Train SR 0.000 | Eval SR 0.000 | Best 0.200 | Patience 3/10\n",
      "Episode   600 | Train SR 0.100 | Eval SR 0.100 | Best 0.200 | Patience 4/10\n",
      "Episode   700 | Train SR 0.100 | Eval SR 0.000 | Best 0.200 | Patience 5/10\n",
      "Episode   800 | Train SR 0.250 | Eval SR 0.300 | Best 0.300 | Patience 0/10\n",
      "Episode   900 | Train SR 0.350 | Eval SR 0.100 | Best 0.300 | Patience 1/10\n",
      "Episode  1000 | Train SR 0.300 | Eval SR 0.100 | Best 0.300 | Patience 2/10\n",
      "Episode  1100 | Train SR 0.100 | Eval SR 0.100 | Best 0.300 | Patience 3/10\n",
      "Episode  1200 | Train SR 0.150 | Eval SR 0.000 | Best 0.300 | Patience 4/10\n",
      "Episode  1300 | Train SR 0.450 | Eval SR 0.400 | Best 0.400 | Patience 0/10\n",
      "Episode  1400 | Train SR 0.450 | Eval SR 0.400 | Best 0.400 | Patience 1/10\n",
      "Episode  1500 | Train SR 0.500 | Eval SR 0.200 | Best 0.400 | Patience 2/10\n",
      "Episode  1600 | Train SR 0.350 | Eval SR 0.300 | Best 0.400 | Patience 3/10\n",
      "Episode  1700 | Train SR 0.550 | Eval SR 0.100 | Best 0.400 | Patience 4/10\n",
      "Episode  1800 | Train SR 0.300 | Eval SR 0.500 | Best 0.500 | Patience 0/10\n",
      "Episode  1900 | Train SR 0.500 | Eval SR 0.300 | Best 0.500 | Patience 1/10\n",
      "Episode  2000 | Train SR 0.400 | Eval SR 0.400 | Best 0.500 | Patience 2/10\n",
      "Episode  2100 | Train SR 0.750 | Eval SR 0.500 | Best 0.500 | Patience 3/10\n",
      "Episode  2200 | Train SR 0.850 | Eval SR 0.200 | Best 0.500 | Patience 4/10\n",
      "Episode  2300 | Train SR 0.800 | Eval SR 0.800 | Best 0.800 | Patience 0/10\n",
      "Episode  2400 | Train SR 0.450 | Eval SR 0.500 | Best 0.800 | Patience 1/10\n",
      "Episode  2500 | Train SR 0.700 | Eval SR 0.800 | Best 0.800 | Patience 2/10\n",
      "Episode  2600 | Train SR 0.900 | Eval SR 0.800 | Best 0.800 | Patience 3/10\n",
      "Episode  2700 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 0/10\n",
      "Episode  2800 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 1/10\n",
      "Episode  2900 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 2/10\n",
      "Episode  3000 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 3/10\n",
      "Episode  3100 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 4/10\n",
      "Episode  3200 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 5/10\n",
      "Episode  3300 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 6/10\n",
      "Episode  3400 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 7/10\n",
      "Episode  3500 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 8/10\n",
      "Episode  3600 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 9/10\n",
      "Episode  3700 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 10/10\n",
      "Early stopping at episode 3700 (no improvement for 10 evals)\n"
     ]
    }
   ],
   "source": [
    "agent_ddpg = run_experiment(\"DDPG\", use_her=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04c8b27",
   "metadata": {
    "papermill": {
     "duration": 0.009266,
     "end_time": "2026-02-16T21:54:26.122727",
     "exception": false,
     "start_time": "2026-02-16T21:54:26.113461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 13.2 DDPG + HER (goal strategy: `final`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd684e06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T21:54:26.142664Z",
     "iopub.status.busy": "2026-02-16T21:54:26.141838Z",
     "iopub.status.idle": "2026-02-16T22:30:57.253138Z",
     "shell.execute_reply": "2026-02-16T22:30:57.252105Z"
    },
    "papermill": {
     "duration": 2191.132316,
     "end_time": "2026-02-16T22:30:57.264078",
     "exception": false,
     "start_time": "2026-02-16T21:54:26.131762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode     0 | Train SR 0.000 | Eval SR 0.100 | Best 0.100 | Patience 0/10\n",
      "Episode   100 | Train SR 0.000 | Eval SR 0.100 | Best 0.100 | Patience 1/10\n",
      "Episode   200 | Train SR 0.650 | Eval SR 1.000 | Best 1.000 | Patience 0/10\n",
      "Episode   300 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 1/10\n",
      "Episode   400 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 2/10\n",
      "Episode   500 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 3/10\n",
      "Episode   600 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 4/10\n",
      "Episode   700 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 5/10\n",
      "Episode   800 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 6/10\n",
      "Episode   900 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 7/10\n",
      "Episode  1000 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 8/10\n",
      "Episode  1100 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 9/10\n",
      "Episode  1200 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 10/10\n",
      "Early stopping at episode 1200 (no improvement for 10 evals)\n"
     ]
    }
   ],
   "source": [
    "agent_her_final = run_experiment(\"DDPG_HER_final\", use_her=True, goal_strategy=\"final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b96f4e",
   "metadata": {
    "papermill": {
     "duration": 0.011092,
     "end_time": "2026-02-16T22:30:57.286490",
     "exception": false,
     "start_time": "2026-02-16T22:30:57.275398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 13.3 DDPG + HER (goal strategy: `episode`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a89fe2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:30:57.310153Z",
     "iopub.status.busy": "2026-02-16T22:30:57.309568Z",
     "iopub.status.idle": "2026-02-16T23:06:03.977241Z",
     "shell.execute_reply": "2026-02-16T23:06:03.976316Z"
    },
    "papermill": {
     "duration": 2106.688597,
     "end_time": "2026-02-16T23:06:03.986151",
     "exception": false,
     "start_time": "2026-02-16T22:30:57.297554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode     0 | Train SR 0.000 | Eval SR 0.000 | Best 0.000 | Patience 0/10\n",
      "Episode   100 | Train SR 0.750 | Eval SR 1.000 | Best 1.000 | Patience 0/10\n",
      "Episode   200 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 1/10\n",
      "Episode   300 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 2/10\n",
      "Episode   400 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 3/10\n",
      "Episode   500 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 4/10\n",
      "Episode   600 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 5/10\n",
      "Episode   700 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 6/10\n",
      "Episode   800 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 7/10\n",
      "Episode   900 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 8/10\n",
      "Episode  1000 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 9/10\n",
      "Episode  1100 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 10/10\n",
      "Early stopping at episode 1100 (no improvement for 10 evals)\n"
     ]
    }
   ],
   "source": [
    "agent_her_episode = run_experiment(\"DDPG_HER_episode\", use_her=True, goal_strategy=\"episode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07454dbd",
   "metadata": {
    "papermill": {
     "duration": 0.009897,
     "end_time": "2026-02-16T23:06:04.005892",
     "exception": false,
     "start_time": "2026-02-16T23:06:03.995995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 13.4 DDPG + HER (goal strategy: `future`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9b5840b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T23:06:04.026964Z",
     "iopub.status.busy": "2026-02-16T23:06:04.026450Z",
     "iopub.status.idle": "2026-02-16T23:44:01.597211Z",
     "shell.execute_reply": "2026-02-16T23:44:01.596384Z"
    },
    "papermill": {
     "duration": 2277.590605,
     "end_time": "2026-02-16T23:44:01.606268",
     "exception": false,
     "start_time": "2026-02-16T23:06:04.015663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode     0 | Train SR 0.000 | Eval SR 0.200 | Best 0.200 | Patience 0/10\n",
      "Episode   100 | Train SR 0.550 | Eval SR 0.800 | Best 0.800 | Patience 0/10\n",
      "Episode   200 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 0/10\n",
      "Episode   300 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 1/10\n",
      "Episode   400 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 2/10\n",
      "Episode   500 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 3/10\n",
      "Episode   600 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 4/10\n",
      "Episode   700 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 5/10\n",
      "Episode   800 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 6/10\n",
      "Episode   900 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 7/10\n",
      "Episode  1000 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 8/10\n",
      "Episode  1100 | Train SR 1.000 | Eval SR 1.000 | Best 1.000 | Patience 9/10\n",
      "Episode  1200 | Train SR 1.000 | Eval SR 0.800 | Best 1.000 | Patience 10/10\n",
      "Early stopping at episode 1200 (no improvement for 10 evals)\n"
     ]
    }
   ],
   "source": [
    "agent_her_future = run_experiment(\"DDPG_HER_future\", use_her=True, goal_strategy=\"future\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a5a5c",
   "metadata": {
    "papermill": {
     "duration": 0.010264,
     "end_time": "2026-02-16T23:44:01.626867",
     "exception": false,
     "start_time": "2026-02-16T23:44:01.616603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8789.308527,
   "end_time": "2026-02-16T23:44:04.821385",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-16T21:17:35.512858",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
